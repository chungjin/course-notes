# Spark

Example: PageRank
```java
// Load graph as an RDD of (URL, outlinks) pairs
val links = spark.textFile(...).map(...).persist()
var ranks = // RDD of (URL, rank) pairs
for (i <- 1 to ITERATIONS) {
  // Build an RDD of (targetURL, float) pairs
  // with the contributions sent by each page
  val contribs = links.join(ranks).flatMap {
  (url, (links, rank)) =>
  links.map(dest => (dest, rank/links.size))
  }
  // Sum contributions by URL and get new ranks
  ranks = contribs.reduceByKey((x,y) => x+y)
  .mapValues(sum => a/N + (1-a)*sum)
}
```

1. Lineage graph
![Lineage graph](https://user-images.githubusercontent.com/11788053/103317590-76d36700-49e0-11eb-8cee-d75c162fd04e.png)
- A graph of transform stages -- a data-flow graph
- Directed acyclic graph(DAG) graph


2. Driver
![Driver](https://user-images.githubusercontent.com/11788053/103317650-abdfb980-49e0-11eb-9894-11abfba5cb0e.png)

- the driver constructs a lineage graph
- the driver compiles Java bytecodes and sends them to worker machines
- the driver then manages execution and data movement



3. Execution
* input in HDFS (like GFS)
* input data files are already "partitioned" over many storage servers
  first 1,000,000 lines in one partition, next lines in another, &c.
* more partitions than machines, for load balance
* each worker machine takes a partition, applies **lineage graph** in order
* when computation on different partitions is independent ("narrow"):
  no inter-machine communication required after first read  
  a worker applies series of transformations to input stream

4. Wide dependencies
![Examples of narrow and wide dependencies](https://user-images.githubusercontent.com/11788053/103318887-a3897d80-49e4-11eb-817e-9dee04c071c5.png)

- after the upstream transformation(map):
  + split output up by shuffle criterion (typically some key)
  + arrange into buckets in memory, one per downstream partition
- before the downstream transformation:
  + (wait until upstream transformation completes -- driver manages this)
  + each worker fetches its bucket from each upstream worker


5. fault tolerance

  One machine crash:
    - driver re-runs transformations on crashed machine's partitions on other machines

  Failure when there are wide dependencies
    - recompute means it need the intermediate results from the partitions among the whole network.
    - Spark supports checkpoints to HDFS (like GFS) to cope with this, don't need to start from the beginning.

## Reference
- [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing, 2012](https://pdos.csail.mit.edu/6.824/papers/zaharia-spark.pdf)
- [mit-course-note: spark](https://pdos.csail.mit.edu/6.824/notes/l-spark.txt)
