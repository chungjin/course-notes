# 9.Aurora SQL

![overall architecture](https://user-images.githubusercontent.com/11788053/95002498-9564c600-0589-11eb-8fd5-faec582f0795.png)

## Durability

### Replication and Correlated Failures
- Node level, a read quorum V_r, write quorum V_w
	+ to ensure each write must aware of the most recent write: `V_w > V/2`
	+  read = max_version(all response), so `V_r + V_w > V`, it can ensure the request must be accepted by a node with most update data. Because each log entry have an index/version, we only pick the date with the most updated log.
- Az level failure tolerance
	+ tolerating (a) losing an entire AZ and one additional node (AZ+1) without losing data
	+ losing an entire AZ without impacting the ability to write data
	+ example, 3 Azs, 2 nodes in each dz
		* V = 6, V1_w = 4, V_r = 3
		* achieve write quorum at least in 2 az
		* read request must have routed to the node with newest data

## The log is the database
offload log processing to the storage service, it helps to reduce network IOs, through minimizing synchronous stalls and unnecessary writes.

### Before offloading
The graph shows the various types of data that the engine needs to write: 1. the redo log, 2. the binary (statement) log that is archived to Amazon Simple Storage Service (S3) in order to support point-in- time restores, 3. the modified data pages, a second temporary write of the data page (double-write) to prevent torn pages, and finally 4. the metadata (FRM) files. The figure also shows the order of the actual IO flow as follows.

1,3,4 is synchronous and block-level software mirroring.

![Network IO in mirrored MySQL](https://user-images.githubusercontent.com/11788053/95002159-e4a8f780-0585-11eb-9678-4696ff2c8510.png)


### Offloading Redo Processing to Storage
![Network IO in Amazon Aurora](https://user-images.githubusercontent.com/11788053/95002172-f38faa00-0585-11eb-8151-a286da3591d2.png)

Only pass the redo log cross the network and wait until achieve write quorum. And storage tier can use it to generate database pages in background on demand.


### Storage Service Design Points
The core design tenet is to minimize the latency of the foreground write request.

![IO Traffic in Aurora Storage Nodes](https://user-images.githubusercontent.com/11788053/95002182-0609e380-0586-11eb-99e9-a87decf13cbe.png)

The activities of the storage node:
1. receive log record and ad to an in-memory queue
2. persist record on disk and acknolwdge
3. organize records and identify gaps in the log since some batches may be lost
4. gossip with peers to fill in gaps
5. coalesce log records into new data pages
6. periodically stage log and new pages to S3
7. periodically garbage collect old versions, and finally
8. periodically validate CRC codes on pages.

All of the above steps are async, and only step 1, 2 will impact the latency.

## Handle read-heavy scenario
read/write ratio can reach to 100. So except for an main DB server, it have \~15 extra standby read server.

The mechanism:
- Read-only replicas read data pages from the storage servers (but don't write). And cache these data pages.
- Main DB server sends log to replicas.
- Read-only Replicas use log to keep cached pages up to date.


Problem and solution:
- Problem: read-only replicas do not know what is locked by transactions.
	+ Solution: replicas ignore or un-do writes of uncommitted transactions. They can tell from commit indications in the log stream.
- Problem: need to prevent replica from seeing B-Tree &c in intermediate state
	+ Solution: atomic mini-transactions within larger transactions.

## Reference
- [Amazon Aurora: Design Considerations for High
Throughput Cloud-Native Relational Databases, 2017](https://pdos.csail.mit.edu/6.824/papers/aurora.pdf)
- [mit course note: aurora](https://pdos.csail.mit.edu/6.824/notes/l-aurora.txt)
