# Cache Consistency, Memcached

## Background
This paper is not for new ideas/techs, it is an experience paper.


### Ways of scaling
A typical story of evolution over time:
1. single machine w/ web server + application + DB
   DB provides persistent storage, crash recovery, transactions, SQL
   application queries DB, formats HTML, &c
   but: As load grows, application takes too much CPU time
2. many web Front End(FEs), one shared DB
   an easy change, since web server + app already separate from storage
   FEs are stateless, all sharing (and concurrency control) via DB
     stateless -> any FE can serve any request, no harm from FE crash
   but: As load grows, need more FEs, soon single DB server is bottleneck
3. many web FEs, data sharded over cluster of DBs
   partition data by key over the DBs
     app looks at key (e.g. user), chooses the right DB
   good DB parallelism if no data is super-popular
   painful
   but: Hot spot issue
   Cross-shard transactions, two phase commit is slow
   slow for read, optimize through cache
4. many web FEs, many caches for reads, many DBs for writes
   cost-effective b/c read-heavy and memcached 10x faster than a DB
     memcached just an in-memory hash table, very simple
   but: Complex b/c DB and memcacheds can get out of sync
   Fragile b/c cache misses can easily overload the DB

## Memcached layer (MC)
1. read and write
![image](https://user-images.githubusercontent.com/11788053/103333146-aeaece80-4a21-11eb-9e1a-2682ba556f9b.png)

FB uses mc as a "look-aside" cache, application determines relationship of mc to DB, mc doesn't know anything about DB
```
read:
  v = get(k) (computes hash(k) to choose mc server)
  if v is nil {
    v = fetch from DB
    set(k, v) to MC
  }
write:
  v = new value
  send k,v to DB
  delete(k) from MC
```

Consistency requirement:
  - user can tolerate modest read staleness
  - want to read-your-own-writes

2. multiple cluster per region
![image](https://user-images.githubusercontent.com/11788053/103337481-66e37380-4a30-11eb-9f2e-5bf8166bef2e.png)
Data is replicated in multiple clusters.

why not add more and more mc servers to a single cluster?
- adding mc servers to cluster doesn't help single popular keys, replicating (one copy per cluster) does help
-  more mcs in cluster -> each client req talks to more servers so all replies come back at the same time
   network switches, NIC run out of buffers
-  hard to build network for single big cluster
   uniform client/server access
   so cross-section b/w must be large -- expensive
   two clusters -> 1/2 the cross-section b/w

3. Regional pool
Replicating is a waste of RAM for less-popular items
![image](https://user-images.githubusercontent.com/11788053/103338197-7a8fd980-4a32-11eb-8c88-69c54d396bb6.png)
So unpopular data is placed in "regional pool", which is shared by all clusters.
Application decide which data is "unpopular".

4. Bring up a new MC
Bringing up new mc cluster is a performance problem.
new cluster has 0% hit rate, if clients use it, will generate big spike in DB load

  Thus the clients of new cluster first data from MC of old cluster and set() into new cluster


5. Thundering herd
  ![Before](https://user-images.githubusercontent.com/11788053/103338530-89c35700-4a33-11eb-962b-b85f87d0114d.png)

  **Problem**: one client updates DB and delete()s a key
    lots of clients get() but miss
    they all fetch from DB
    they all set()

    ![after](https://user-images.githubusercontent.com/11788053/103338566-a2cc0800-4a33-11eb-83eb-cfbfefb49d76.png)
  **Solution**: mc gives just the first missing client a "lease"
    lease = permission to refresh from DB
    mc tells others "try get() again in a few milliseconds"
  **Effect**: only one client reads the DB and does set()
      others re-try get() later and hopefully hit  

## Consistency
Write:
  - writes go direct to primary DB, with transactions, so writes are consistent

Read:
  - reads do not always see the latest write
  more like "not more than a few seconds stale"
  *and* writers see their own writes (due to delete())


Race 1: k not in cache
```
C1 get(k), misses, grant lease
C1 v1 = read k from DB
  C2 writes k = v2 in DB
  C2 delete(k), remove lease
C1 set(k, v1), found lease invalid, abort set
```

Race 2: during cold cluster warm-up
remember: on miss, clients try get() in warm cluster, copy to cold cluster
```
C1 updates k to v2 in DB
C1 delete(k) -- in cold cluster
  C2 get(k), miss -- in cold cluster
  C2 v1 = get(k) from warm cluster, hits
C2 set(k, v1) into cold cluster
```
Problem: as delete() already happend, stay stale v1 indefinitely, until key is next written
Solution: with **two-second hold-off**, just used on **cold** clusters
  after C1 delete(), cold mc ignores set()s for two seconds
  by then, delete() will (probably) propagate via DB to warm cluster

## Reference
- [Scaling Memcache at Facebook, 2013](https://pdos.csail.mit.edu/6.824/papers/memcache-fb.pdf)
- [mit-course-note: Cache Consistency, Memcached at Facebook](https://pdos.csail.mit.edu/6.824/notes/l-memcached.txt)
