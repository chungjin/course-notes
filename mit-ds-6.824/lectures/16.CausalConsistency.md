# Lecture 16: Causal Consistency, COPS


## Background
the setting: geo-replication for big web sites
- multiple datacenters
- each datacenter has a complete copy of all data
- reads are local -- fast common case
- what about writes?
- what about consistency?

Solution 1: Spanner
- Insist on strong consistency and Paxos quorum for write, it means it must wait for some remote sites
- Optimize for read from local, through version validation.

Solution 2: Facebook / Memcache
- writes must go to the primary site's MySQL
- but reads are blindingly fast (1,000,000 per second per memcache server)

Possible improvements?
- allows writes from any datacenter?


## One straw-man
Eventually consistency:
- client reads and writes just locally(achive local quorum)
- each shard pushes writes to other datacenters, shard-to-shard, asynchronously
Problem:

Problem:
- clients may see updates in different orders

Race Example:
```
C1 uploads photo, adds reference to public list:
  C1: put(photo) put(list)
C2 reads:
  C2:                       get(list) get(photo)
C3 also sees new photo, adds to their own list:
  C3: get(list) put(list2)
C4 sees photo on C3's list:
```

## Two straw-man
Provide a `sync(k, v#)` operation, `sync()` does not return until every datacenter has at least v# for k.


Example:
```
C1: v# = put(photo), sync(photo, v#), put(list)
C2:                                             get(list) get(photo)
C2 may not see the new list, but if it does, it will see photo too
```

How to avoid write cost?
- single write log per datacenter, send log in order to other datacenter.
- but the log server might be a bottleneck if there are many shards

## Causal consistency
each COPS client maintains a "context" to reflect order of client ops
```
get(X)->v2
  context: Xv2
get(Y)->v4
  context: Xv2, Yv4
put(Z, -)->v3
  client sends Xv2, Yv4 to shard server along with new Z
  context: Xv2, Yv4, Zv3
```

when it receives a put(Z, -, Yv4) from a client,
```
    picks a new v# = 3 for Z,
    stores Z, -, v3
    sends Z/-/v3/Yv4 to corresponding shard server in each datacenter
      but does not wait for reply
  remote shard server receives Z/-/v3/Yv4
    talks to local shard server for Y
      waits for Yv4 to arrive
    then updates DB to hold Z/-/v3
```    

Limitation/drawbacks:
- conflicting writes are a serious difficulty
- awkward for clients to track causality
  e.g. user and browser, multiple page views, multiple servers
- COPS doesn't see external causal dependencies
  s/w and people really do communicate outside of the COPS world
- limited notion of "transaction"
  only for reads (though later work generalized a bit)
  definition is more subtle than serializable transactions
- significant overhead to track, communicate, obey causal dependencies
  remote servers must check and delay updates
  update delays may cascade

## Retrospect
Causal consistency is rarely used in deployed storage systems
what is actually used?
- no geographic replication at all, just local
- primary-site (PNUTS, Facebook/Memcache)
- eventual consistency (Dynamo, Cassandra)
- strongly consistent (Spanner)

## Reference
- Lloyd et al, Don't Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS, SOSP 2011.
- [mit-course-note](https://pdos.csail.mit.edu/6.824/notes/l-cops.txt)
