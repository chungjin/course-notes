# 7.Zookeeper


ZooKeeper, a service for coordinating processes of distributed applications. It can maintain configuration information, naming, providing distributed synchronization, and providing group services, similar to distributed file system.

- Configuration information
   + config file is rarely modified but read heavily
   + each nodes listen to the change, it will get notified if config file is modified.
- Naming service
   + Mapping a name with the IP(similar to DNS service)
   + Within a clueter, there will be multiple nodes provide the same service. When the provider join to the cluster, it will register the information(endpoint, IP address, ports) to Zookeeper. When the consumer comes in and consume the service, the zookeeper can return the address and can achieve the load balancer at the same time.
- Distributed lock
   + Synchronized the progress, and guarantee the linearizability of the requests.        



For a read-heavy application, if we insist on linearizability, it means we need to persist the read log also, populated to the majority and response to the user. So zookeeper relax this restriction, it allows reads to yield stale data, so each replica can also reply to the client for read request.

## Why zookeeper?
- Fault-tolerance general purpose coordinate service.
- Widely applied for meta-data used for coordination purpose.

## Overview

znode denote an in-memory data node in the ZooKeeper data, which is organized in a hierarchical namespace referred to as the data tree.
![znode architecture](https://user-images.githubusercontent.com/11788053/87625377-e37ed180-c6de-11ea-9c86-ebd28c402623.png)

The hierarchal namespace is useful for allocating subtrees for the namespace of different **applications** and for setting access rights to those subtrees. 


Each client process P_i creates a znode p_i under /app1, which persists as long the process is running.

znodes also carry the information of timestamp and version counters, which allow clients to track changes to znodes and execute conditional updates based on the version of the znode.

The log of write request will be replicated among the majority, The log of read request will only persist in the replica that contact with the client.


## Ordering guarantees
1. Linearizable writes
	+ clients send writes to the leader the leader chooses an order, numbered by "zxid" sends to replicas, which all execute in zxid order
2. FIFO client order
	+ each client specifies an order for its operations (reads AND writes)
	+ write:
		* writes appear in the write order in client-specified order
	+ reads
		* Linearizability reside in each client internally.
		* a client's read executes after all previous writes by that client. A server may block a client's read to wait for previous write, or `sync()`

- What if A client want to see the B client config change? - ready file(similar to sync() call)
```
    e.g. if read sees "ready" file, subsequent reads see previous writes.
         (Section 2.3)
         Client B Write order:      Client A Read order:
         delete("ready")
         write f1
         write f2
         create("ready")
			                        exists("ready")
			                        read f1
			                        read f2
```
- ready file changes in the middle of a sequence of read request
	+ Solution: zookeeper API, set `watch = true` flag, whenever the file get modified, get notification.
		* the replica will have a watch table, if the replica crash and relaunch, it will have a stand by replica, the watch table will be recovered.
```
         Write order:      Read order:
                           exists("ready", watch=true)
                           read f1
         delete("ready")
         write f1
         write f2
                           read f2
```

## Consequences
- Leader must preserve client write order across leader failure.
- Replicas must enforce "a client's reads never go backwards in zxid order", despite replica failure.
- Client must track highest zxid it has read to help ensure next read doesn't go backwards even if sent to a different replica.

## Other performance trick
- Clients can send async writes to leader (async = don't have to wait).
- Leader batches up many requests to reduce net and disk-write overhead (Assumes lots of active clients)
- Fuzzy snapshots (and idempotent updates) so snapshot doesn't stop writes.



## Reference
- [Introduction of zookeeper](https://juejin.im/post/5bac3cde5188255c3b7d91a8)

